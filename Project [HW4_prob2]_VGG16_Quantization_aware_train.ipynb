{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ef1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [1, 13]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a087ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.812 (0.812)\tData 0.211 (0.211)\tLoss 2.5745 (2.5745)\tPrec 13.281% (13.281%)\n",
      "Epoch: [0][100/391]\tTime 0.038 (0.047)\tData 0.002 (0.004)\tLoss 2.1663 (2.1412)\tPrec 24.219% (22.633%)\n",
      "Epoch: [0][200/391]\tTime 0.037 (0.044)\tData 0.002 (0.003)\tLoss 1.5533 (1.9276)\tPrec 41.406% (29.027%)\n",
      "Epoch: [0][300/391]\tTime 0.039 (0.042)\tData 0.001 (0.002)\tLoss 1.3135 (1.7955)\tPrec 47.656% (33.576%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 1.5074 (1.5074)\tPrec 42.969% (42.969%)\n",
      " * Prec 47.680% \n",
      "best acc: 47.680000\n",
      "Epoch: [1][0/391]\tTime 0.238 (0.238)\tData 0.184 (0.184)\tLoss 1.3029 (1.3029)\tPrec 49.219% (49.219%)\n",
      "Epoch: [1][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 1.3030 (1.2854)\tPrec 50.781% (52.259%)\n",
      "Epoch: [1][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 1.2450 (1.2497)\tPrec 59.375% (54.264%)\n",
      "Epoch: [1][300/391]\tTime 0.045 (0.040)\tData 0.002 (0.002)\tLoss 1.0069 (1.2156)\tPrec 60.156% (55.733%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 1.2096 (1.2096)\tPrec 54.688% (54.688%)\n",
      " * Prec 57.880% \n",
      "best acc: 57.880000\n",
      "Epoch: [2][0/391]\tTime 0.233 (0.233)\tData 0.195 (0.195)\tLoss 0.9372 (0.9372)\tPrec 67.188% (67.188%)\n",
      "Epoch: [2][100/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.7774 (1.0394)\tPrec 73.438% (62.508%)\n",
      "Epoch: [2][200/391]\tTime 0.035 (0.041)\tData 0.001 (0.003)\tLoss 0.8281 (1.0130)\tPrec 66.406% (63.825%)\n",
      "Epoch: [2][300/391]\tTime 0.042 (0.041)\tData 0.002 (0.002)\tLoss 0.8522 (0.9969)\tPrec 67.969% (64.395%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.8815 (0.8815)\tPrec 67.969% (67.969%)\n",
      " * Prec 65.330% \n",
      "best acc: 65.330000\n",
      "Epoch: [3][0/391]\tTime 0.228 (0.228)\tData 0.190 (0.190)\tLoss 0.8520 (0.8520)\tPrec 71.094% (71.094%)\n",
      "Epoch: [3][100/391]\tTime 0.041 (0.042)\tData 0.001 (0.004)\tLoss 0.7987 (0.8937)\tPrec 73.438% (68.178%)\n",
      "Epoch: [3][200/391]\tTime 0.037 (0.040)\tData 0.002 (0.003)\tLoss 0.8929 (0.8723)\tPrec 67.969% (69.096%)\n",
      "Epoch: [3][300/391]\tTime 0.038 (0.040)\tData 0.001 (0.002)\tLoss 0.9549 (0.8568)\tPrec 64.844% (69.669%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.8489 (0.8489)\tPrec 67.188% (67.188%)\n",
      " * Prec 68.760% \n",
      "best acc: 68.760000\n",
      "Epoch: [4][0/391]\tTime 0.294 (0.294)\tData 0.252 (0.252)\tLoss 0.7779 (0.7779)\tPrec 72.656% (72.656%)\n",
      "Epoch: [4][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.6629 (0.7847)\tPrec 75.781% (72.672%)\n",
      "Epoch: [4][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.9895 (0.7793)\tPrec 64.844% (72.839%)\n",
      "Epoch: [4][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.7467 (0.7684)\tPrec 68.750% (73.162%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.7761 (0.7761)\tPrec 71.875% (71.875%)\n",
      " * Prec 73.150% \n",
      "best acc: 73.150000\n",
      "Epoch: [5][0/391]\tTime 0.271 (0.271)\tData 0.228 (0.228)\tLoss 0.7182 (0.7182)\tPrec 70.312% (70.312%)\n",
      "Epoch: [5][100/391]\tTime 0.043 (0.042)\tData 0.002 (0.004)\tLoss 0.6926 (0.7113)\tPrec 75.000% (75.302%)\n",
      "Epoch: [5][200/391]\tTime 0.035 (0.041)\tData 0.002 (0.003)\tLoss 0.7021 (0.7039)\tPrec 73.438% (75.634%)\n",
      "Epoch: [5][300/391]\tTime 0.038 (0.040)\tData 0.001 (0.002)\tLoss 0.7422 (0.7022)\tPrec 73.438% (75.836%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.7551 (0.7551)\tPrec 74.219% (74.219%)\n",
      " * Prec 75.070% \n",
      "best acc: 75.070000\n",
      "Epoch: [6][0/391]\tTime 0.246 (0.246)\tData 0.196 (0.196)\tLoss 0.5644 (0.5644)\tPrec 79.688% (79.688%)\n",
      "Epoch: [6][100/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.6444 (0.6537)\tPrec 79.688% (77.785%)\n",
      "Epoch: [6][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.5403 (0.6480)\tPrec 84.375% (77.993%)\n",
      "Epoch: [6][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.002)\tLoss 0.7182 (0.6476)\tPrec 76.562% (78.045%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.6857 (0.6857)\tPrec 71.094% (71.094%)\n",
      " * Prec 76.740% \n",
      "best acc: 76.740000\n",
      "Epoch: [7][0/391]\tTime 0.254 (0.254)\tData 0.209 (0.209)\tLoss 0.6639 (0.6639)\tPrec 75.000% (75.000%)\n",
      "Epoch: [7][100/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.6848 (0.6154)\tPrec 77.344% (78.929%)\n",
      "Epoch: [7][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.6336 (0.6182)\tPrec 78.125% (78.809%)\n",
      "Epoch: [7][300/391]\tTime 0.040 (0.041)\tData 0.003 (0.002)\tLoss 0.5855 (0.6129)\tPrec 78.906% (78.961%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.5881 (0.5881)\tPrec 78.125% (78.125%)\n",
      " * Prec 78.250% \n",
      "best acc: 78.250000\n",
      "Epoch: [8][0/391]\tTime 0.263 (0.263)\tData 0.222 (0.222)\tLoss 0.4660 (0.4660)\tPrec 82.031% (82.031%)\n",
      "Epoch: [8][100/391]\tTime 0.038 (0.043)\tData 0.001 (0.004)\tLoss 0.5858 (0.5696)\tPrec 80.469% (80.515%)\n",
      "Epoch: [8][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.5609 (0.5760)\tPrec 82.031% (80.115%)\n",
      "Epoch: [8][300/391]\tTime 0.042 (0.041)\tData 0.002 (0.003)\tLoss 0.5460 (0.5785)\tPrec 80.469% (80.170%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.426 (0.426)\tLoss 0.5782 (0.5782)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.150% \n",
      "best acc: 80.150000\n",
      "Epoch: [9][0/391]\tTime 0.258 (0.258)\tData 0.214 (0.214)\tLoss 0.6318 (0.6318)\tPrec 76.562% (76.562%)\n",
      "Epoch: [9][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.6868 (0.5465)\tPrec 75.781% (81.443%)\n",
      "Epoch: [9][200/391]\tTime 0.041 (0.041)\tData 0.001 (0.003)\tLoss 0.5363 (0.5473)\tPrec 79.688% (81.514%)\n",
      "Epoch: [9][300/391]\tTime 0.037 (0.040)\tData 0.002 (0.002)\tLoss 0.5724 (0.5489)\tPrec 82.812% (81.346%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.6202 (0.6202)\tPrec 80.469% (80.469%)\n",
      " * Prec 77.150% \n",
      "best acc: 80.150000\n",
      "Epoch: [10][0/391]\tTime 0.301 (0.301)\tData 0.258 (0.258)\tLoss 0.4672 (0.4672)\tPrec 85.156% (85.156%)\n",
      "Epoch: [10][100/391]\tTime 0.038 (0.042)\tData 0.002 (0.004)\tLoss 0.5168 (0.5127)\tPrec 79.688% (82.495%)\n",
      "Epoch: [10][200/391]\tTime 0.038 (0.040)\tData 0.001 (0.003)\tLoss 0.5517 (0.5197)\tPrec 80.469% (82.206%)\n",
      "Epoch: [10][300/391]\tTime 0.037 (0.040)\tData 0.002 (0.002)\tLoss 0.4332 (0.5163)\tPrec 87.500% (82.319%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.193 (0.193)\tLoss 0.6948 (0.6948)\tPrec 74.219% (74.219%)\n",
      " * Prec 76.460% \n",
      "best acc: 80.150000\n",
      "Epoch: [11][0/391]\tTime 0.255 (0.255)\tData 0.210 (0.210)\tLoss 0.3816 (0.3816)\tPrec 87.500% (87.500%)\n",
      "Epoch: [11][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.4095 (0.4931)\tPrec 83.594% (83.184%)\n",
      "Epoch: [11][200/391]\tTime 0.040 (0.041)\tData 0.001 (0.003)\tLoss 0.4288 (0.4929)\tPrec 85.156% (83.092%)\n",
      "Epoch: [11][300/391]\tTime 0.040 (0.040)\tData 0.001 (0.002)\tLoss 0.6658 (0.4921)\tPrec 77.344% (83.153%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.5338 (0.5338)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.020% \n",
      "best acc: 81.020000\n",
      "Epoch: [12][0/391]\tTime 0.265 (0.265)\tData 0.218 (0.218)\tLoss 0.4310 (0.4310)\tPrec 83.594% (83.594%)\n",
      "Epoch: [12][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.3551 (0.4620)\tPrec 87.500% (84.050%)\n",
      "Epoch: [12][200/391]\tTime 0.041 (0.041)\tData 0.001 (0.003)\tLoss 0.6576 (0.4725)\tPrec 75.000% (83.850%)\n",
      "Epoch: [12][300/391]\tTime 0.040 (0.040)\tData 0.001 (0.002)\tLoss 0.5285 (0.4695)\tPrec 81.250% (84.022%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.5357 (0.5357)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.460% \n",
      "best acc: 81.460000\n",
      "Epoch: [13][0/391]\tTime 0.263 (0.263)\tData 0.217 (0.217)\tLoss 0.4361 (0.4361)\tPrec 85.938% (85.938%)\n",
      "Epoch: [13][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.4120 (0.4371)\tPrec 83.594% (85.040%)\n",
      "Epoch: [13][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.4107 (0.4506)\tPrec 87.500% (84.694%)\n",
      "Epoch: [13][300/391]\tTime 0.039 (0.040)\tData 0.002 (0.002)\tLoss 0.5631 (0.4534)\tPrec 80.469% (84.679%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.5858 (0.5858)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.380% \n",
      "best acc: 81.460000\n",
      "Epoch: [14][0/391]\tTime 0.264 (0.264)\tData 0.223 (0.223)\tLoss 0.4650 (0.4650)\tPrec 84.375% (84.375%)\n",
      "Epoch: [14][100/391]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 0.4529 (0.4204)\tPrec 80.469% (85.845%)\n",
      "Epoch: [14][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.5933 (0.4265)\tPrec 78.906% (85.798%)\n",
      "Epoch: [14][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.4569 (0.4321)\tPrec 83.594% (85.463%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.4219 (0.4219)\tPrec 87.500% (87.500%)\n",
      " * Prec 82.970% \n",
      "best acc: 82.970000\n",
      "Epoch: [15][0/391]\tTime 0.242 (0.242)\tData 0.200 (0.200)\tLoss 0.5069 (0.5069)\tPrec 85.156% (85.156%)\n",
      "Epoch: [15][100/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.2943 (0.4112)\tPrec 91.406% (86.092%)\n",
      "Epoch: [15][200/391]\tTime 0.044 (0.041)\tData 0.002 (0.002)\tLoss 0.5687 (0.4171)\tPrec 77.344% (85.840%)\n",
      "Epoch: [15][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.4715 (0.4188)\tPrec 86.719% (85.800%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.5041 (0.5041)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.220% \n",
      "best acc: 83.220000\n",
      "Epoch: [16][0/391]\tTime 0.250 (0.250)\tData 0.219 (0.219)\tLoss 0.2388 (0.2388)\tPrec 91.406% (91.406%)\n",
      "Epoch: [16][100/391]\tTime 0.041 (0.042)\tData 0.001 (0.004)\tLoss 0.5103 (0.3862)\tPrec 82.812% (86.873%)\n",
      "Epoch: [16][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.4073 (0.3951)\tPrec 87.500% (86.598%)\n",
      "Epoch: [16][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.4357 (0.4031)\tPrec 85.156% (86.371%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.5611 (0.5611)\tPrec 79.688% (79.688%)\n",
      " * Prec 82.830% \n",
      "best acc: 83.220000\n",
      "Epoch: [17][0/391]\tTime 0.241 (0.241)\tData 0.196 (0.196)\tLoss 0.2800 (0.2800)\tPrec 90.625% (90.625%)\n",
      "Epoch: [17][100/391]\tTime 0.041 (0.043)\tData 0.002 (0.004)\tLoss 0.3978 (0.3747)\tPrec 88.281% (87.098%)\n",
      "Epoch: [17][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.5033 (0.3864)\tPrec 83.594% (86.835%)\n",
      "Epoch: [17][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.4560 (0.3872)\tPrec 85.938% (86.869%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.5688 (0.5688)\tPrec 79.688% (79.688%)\n",
      " * Prec 81.960% \n",
      "best acc: 83.220000\n",
      "Epoch: [18][0/391]\tTime 0.264 (0.264)\tData 0.215 (0.215)\tLoss 0.2802 (0.2802)\tPrec 89.062% (89.062%)\n",
      "Epoch: [18][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.5185 (0.3736)\tPrec 85.938% (87.361%)\n",
      "Epoch: [18][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.5175 (0.3767)\tPrec 83.594% (87.119%)\n",
      "Epoch: [18][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.3082 (0.3757)\tPrec 90.625% (87.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.4236 (0.4236)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.560% \n",
      "best acc: 83.220000\n",
      "Epoch: [19][0/391]\tTime 0.300 (0.300)\tData 0.255 (0.255)\tLoss 0.3720 (0.3720)\tPrec 85.156% (85.156%)\n",
      "Epoch: [19][100/391]\tTime 0.038 (0.042)\tData 0.002 (0.004)\tLoss 0.3900 (0.3690)\tPrec 88.281% (87.771%)\n",
      "Epoch: [19][200/391]\tTime 0.039 (0.040)\tData 0.002 (0.003)\tLoss 0.3032 (0.3595)\tPrec 88.281% (87.792%)\n",
      "Epoch: [19][300/391]\tTime 0.039 (0.040)\tData 0.002 (0.003)\tLoss 0.3703 (0.3597)\tPrec 87.500% (87.871%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.4531 (0.4531)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.390% \n",
      "best acc: 83.390000\n",
      "Epoch: [20][0/391]\tTime 0.245 (0.245)\tData 0.208 (0.208)\tLoss 0.2939 (0.2939)\tPrec 88.281% (88.281%)\n",
      "Epoch: [20][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.2260 (0.2846)\tPrec 92.969% (90.292%)\n",
      "Epoch: [20][200/391]\tTime 0.042 (0.041)\tData 0.002 (0.003)\tLoss 0.1415 (0.2704)\tPrec 96.094% (90.804%)\n",
      "Epoch: [20][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.3019 (0.2566)\tPrec 91.406% (91.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.2150 (0.2150)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.030% \n",
      "best acc: 89.030000\n",
      "Epoch: [21][0/391]\tTime 0.320 (0.320)\tData 0.278 (0.278)\tLoss 0.2695 (0.2695)\tPrec 92.188% (92.188%)\n",
      "Epoch: [21][100/391]\tTime 0.045 (0.044)\tData 0.002 (0.005)\tLoss 0.2400 (0.2205)\tPrec 92.969% (92.636%)\n",
      "Epoch: [21][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.003)\tLoss 0.1216 (0.2156)\tPrec 94.531% (92.712%)\n",
      "Epoch: [21][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.003)\tLoss 0.2193 (0.2147)\tPrec 93.750% (92.787%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.2058 (0.2058)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.200% \n",
      "best acc: 89.200000\n",
      "Epoch: [22][0/391]\tTime 0.251 (0.251)\tData 0.198 (0.198)\tLoss 0.1957 (0.1957)\tPrec 92.969% (92.969%)\n",
      "Epoch: [22][100/391]\tTime 0.045 (0.044)\tData 0.002 (0.004)\tLoss 0.2372 (0.2003)\tPrec 92.969% (93.108%)\n",
      "Epoch: [22][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.003)\tLoss 0.2459 (0.2036)\tPrec 92.188% (93.101%)\n",
      "Epoch: [22][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.003)\tLoss 0.2106 (0.2064)\tPrec 93.750% (93.031%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.1516 (0.1516)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.230% \n",
      "best acc: 89.230000\n",
      "Epoch: [23][0/391]\tTime 0.268 (0.268)\tData 0.223 (0.223)\tLoss 0.1583 (0.1583)\tPrec 93.750% (93.750%)\n",
      "Epoch: [23][100/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.2487 (0.1947)\tPrec 91.406% (93.673%)\n",
      "Epoch: [23][200/391]\tTime 0.038 (0.043)\tData 0.001 (0.003)\tLoss 0.2069 (0.1946)\tPrec 92.969% (93.427%)\n",
      "Epoch: [23][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.003)\tLoss 0.1604 (0.1920)\tPrec 94.531% (93.444%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.1880 (0.1880)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.160% \n",
      "best acc: 89.230000\n",
      "Epoch: [24][0/391]\tTime 0.252 (0.252)\tData 0.203 (0.203)\tLoss 0.1725 (0.1725)\tPrec 95.312% (95.312%)\n",
      "Epoch: [24][100/391]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 0.1025 (0.1790)\tPrec 96.875% (93.967%)\n",
      "Epoch: [24][200/391]\tTime 0.039 (0.041)\tData 0.003 (0.003)\tLoss 0.1408 (0.1779)\tPrec 94.531% (94.053%)\n",
      "Epoch: [24][300/391]\tTime 0.044 (0.041)\tData 0.002 (0.003)\tLoss 0.1424 (0.1793)\tPrec 95.312% (94.051%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.189 (0.189)\tLoss 0.1702 (0.1702)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.440% \n",
      "best acc: 89.440000\n",
      "Epoch: [25][0/391]\tTime 0.275 (0.275)\tData 0.227 (0.227)\tLoss 0.1954 (0.1954)\tPrec 93.750% (93.750%)\n",
      "Epoch: [25][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.1575 (0.1740)\tPrec 94.531% (94.268%)\n",
      "Epoch: [25][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.2693 (0.1755)\tPrec 92.969% (94.076%)\n",
      "Epoch: [25][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.1823 (0.1752)\tPrec 94.531% (94.090%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.2040 (0.2040)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.710% \n",
      "best acc: 89.710000\n",
      "Epoch: [26][0/391]\tTime 0.248 (0.248)\tData 0.203 (0.203)\tLoss 0.1598 (0.1598)\tPrec 93.750% (93.750%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.043)\tData 0.002 (0.004)\tLoss 0.2061 (0.1704)\tPrec 92.188% (94.144%)\n",
      "Epoch: [26][200/391]\tTime 0.047 (0.043)\tData 0.003 (0.003)\tLoss 0.1184 (0.1707)\tPrec 96.094% (94.131%)\n",
      "Epoch: [26][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.003)\tLoss 0.2586 (0.1716)\tPrec 92.969% (94.142%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.1926 (0.1926)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.080% \n",
      "best acc: 90.080000\n",
      "Epoch: [27][0/391]\tTime 0.252 (0.252)\tData 0.213 (0.213)\tLoss 0.1301 (0.1301)\tPrec 96.094% (96.094%)\n",
      "Epoch: [27][100/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.2175 (0.1667)\tPrec 92.969% (94.199%)\n",
      "Epoch: [27][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.0938 (0.1643)\tPrec 96.094% (94.407%)\n",
      "Epoch: [27][300/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.1040 (0.1668)\tPrec 96.094% (94.277%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.2265 (0.2265)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.740% \n",
      "best acc: 90.080000\n",
      "Epoch: [28][0/391]\tTime 0.317 (0.317)\tData 0.274 (0.274)\tLoss 0.2133 (0.2133)\tPrec 94.531% (94.531%)\n",
      "Epoch: [28][100/391]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 0.1453 (0.1610)\tPrec 93.750% (94.500%)\n",
      "Epoch: [28][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.1383 (0.1590)\tPrec 96.875% (94.648%)\n",
      "Epoch: [28][300/391]\tTime 0.041 (0.041)\tData 0.001 (0.003)\tLoss 0.1376 (0.1557)\tPrec 96.094% (94.723%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.1784 (0.1784)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.880% \n",
      "best acc: 90.080000\n",
      "Epoch: [29][0/391]\tTime 0.230 (0.230)\tData 0.188 (0.188)\tLoss 0.0932 (0.0932)\tPrec 96.094% (96.094%)\n",
      "Epoch: [29][100/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.1565 (0.1465)\tPrec 95.312% (95.150%)\n",
      "Epoch: [29][200/391]\tTime 0.044 (0.041)\tData 0.001 (0.003)\tLoss 0.1464 (0.1504)\tPrec 96.094% (95.017%)\n",
      "Epoch: [29][300/391]\tTime 0.039 (0.041)\tData 0.002 (0.002)\tLoss 0.1332 (0.1514)\tPrec 94.531% (94.960%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.1775 (0.1775)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.640% \n",
      "best acc: 90.080000\n",
      "Epoch: [30][0/391]\tTime 0.249 (0.249)\tData 0.207 (0.207)\tLoss 0.1539 (0.1539)\tPrec 94.531% (94.531%)\n",
      "Epoch: [30][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.1116 (0.1485)\tPrec 96.094% (95.150%)\n",
      "Epoch: [30][200/391]\tTime 0.038 (0.040)\tData 0.001 (0.003)\tLoss 0.2282 (0.1508)\tPrec 92.188% (95.114%)\n",
      "Epoch: [30][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.1157 (0.1503)\tPrec 94.531% (95.081%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.201 (0.201)\tLoss 0.2881 (0.2881)\tPrec 89.844% (89.844%)\n",
      " * Prec 89.750% \n",
      "best acc: 90.080000\n",
      "Epoch: [31][0/391]\tTime 0.251 (0.251)\tData 0.213 (0.213)\tLoss 0.1343 (0.1343)\tPrec 96.094% (96.094%)\n",
      "Epoch: [31][100/391]\tTime 0.035 (0.043)\tData 0.001 (0.004)\tLoss 0.1427 (0.1511)\tPrec 96.875% (94.949%)\n",
      "Epoch: [31][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 0.0995 (0.1490)\tPrec 96.875% (95.002%)\n",
      "Epoch: [31][300/391]\tTime 0.039 (0.041)\tData 0.001 (0.002)\tLoss 0.1378 (0.1473)\tPrec 96.094% (95.019%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.2168 (0.2168)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.810% \n",
      "best acc: 90.080000\n",
      "Epoch: [32][0/391]\tTime 0.223 (0.223)\tData 0.179 (0.179)\tLoss 0.2040 (0.2040)\tPrec 92.188% (92.188%)\n",
      "Epoch: [32][100/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.2481 (0.1347)\tPrec 89.844% (95.444%)\n",
      "Epoch: [32][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.003)\tLoss 0.1926 (0.1380)\tPrec 93.750% (95.355%)\n",
      "Epoch: [32][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.0842 (0.1410)\tPrec 96.094% (95.255%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.2177 (0.2177)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.870% \n",
      "best acc: 90.080000\n",
      "Epoch: [33][0/391]\tTime 0.258 (0.258)\tData 0.215 (0.215)\tLoss 0.1041 (0.1041)\tPrec 96.094% (96.094%)\n",
      "Epoch: [33][100/391]\tTime 0.041 (0.043)\tData 0.002 (0.004)\tLoss 0.1019 (0.1372)\tPrec 95.312% (95.367%)\n",
      "Epoch: [33][200/391]\tTime 0.036 (0.041)\tData 0.002 (0.003)\tLoss 0.1784 (0.1413)\tPrec 93.750% (95.204%)\n",
      "Epoch: [33][300/391]\tTime 0.043 (0.041)\tData 0.002 (0.003)\tLoss 0.1607 (0.1404)\tPrec 92.969% (95.196%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.1573 (0.1573)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.780% \n",
      "best acc: 90.080000\n",
      "Epoch: [34][0/391]\tTime 0.254 (0.254)\tData 0.204 (0.204)\tLoss 0.1997 (0.1997)\tPrec 94.531% (94.531%)\n",
      "Epoch: [34][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.1903 (0.1397)\tPrec 95.312% (95.119%)\n",
      "Epoch: [34][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.1045 (0.1390)\tPrec 96.094% (95.231%)\n",
      "Epoch: [34][300/391]\tTime 0.040 (0.041)\tData 0.002 (0.002)\tLoss 0.1839 (0.1372)\tPrec 96.094% (95.325%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.1967 (0.1967)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.930% \n",
      "best acc: 90.080000\n",
      "Epoch: [35][0/391]\tTime 0.260 (0.260)\tData 0.217 (0.217)\tLoss 0.1092 (0.1092)\tPrec 96.094% (96.094%)\n",
      "Epoch: [35][100/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0507 (0.1187)\tPrec 98.438% (95.908%)\n",
      "Epoch: [35][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.1380 (0.1244)\tPrec 96.094% (95.806%)\n",
      "Epoch: [35][300/391]\tTime 0.039 (0.041)\tData 0.001 (0.002)\tLoss 0.1298 (0.1266)\tPrec 94.531% (95.728%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.1789 (0.1789)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.060% \n",
      "best acc: 90.080000\n",
      "Epoch: [36][0/391]\tTime 0.234 (0.234)\tData 0.184 (0.184)\tLoss 0.2089 (0.2089)\tPrec 93.750% (93.750%)\n",
      "Epoch: [36][100/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.1204 (0.1279)\tPrec 95.312% (95.630%)\n",
      "Epoch: [36][200/391]\tTime 0.038 (0.042)\tData 0.001 (0.003)\tLoss 0.0594 (0.1276)\tPrec 96.875% (95.608%)\n",
      "Epoch: [36][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.002)\tLoss 0.1235 (0.1270)\tPrec 95.312% (95.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.2469 (0.2469)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.530% \n",
      "best acc: 90.080000\n",
      "Epoch: [37][0/391]\tTime 0.246 (0.246)\tData 0.199 (0.199)\tLoss 0.1253 (0.1253)\tPrec 94.531% (94.531%)\n",
      "Epoch: [37][100/391]\tTime 0.040 (0.041)\tData 0.001 (0.003)\tLoss 0.1535 (0.1172)\tPrec 93.750% (95.908%)\n",
      "Epoch: [37][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.0561 (0.1203)\tPrec 98.438% (95.818%)\n",
      "Epoch: [37][300/391]\tTime 0.038 (0.040)\tData 0.001 (0.002)\tLoss 0.1285 (0.1220)\tPrec 96.094% (95.741%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.2563 (0.2563)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.000% \n",
      "best acc: 90.080000\n",
      "Epoch: [38][0/391]\tTime 0.333 (0.333)\tData 0.282 (0.282)\tLoss 0.1622 (0.1622)\tPrec 96.094% (96.094%)\n",
      "Epoch: [38][100/391]\tTime 0.038 (0.043)\tData 0.001 (0.004)\tLoss 0.1118 (0.1158)\tPrec 95.312% (96.047%)\n",
      "Epoch: [38][200/391]\tTime 0.042 (0.042)\tData 0.001 (0.003)\tLoss 0.1360 (0.1184)\tPrec 95.312% (95.845%)\n",
      "Epoch: [38][300/391]\tTime 0.041 (0.041)\tData 0.001 (0.002)\tLoss 0.1763 (0.1229)\tPrec 92.969% (95.676%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.1813 (0.1813)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.200% \n",
      "best acc: 90.200000\n",
      "Epoch: [39][0/391]\tTime 0.286 (0.286)\tData 0.243 (0.243)\tLoss 0.1152 (0.1152)\tPrec 96.875% (96.875%)\n",
      "Epoch: [39][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0891 (0.1063)\tPrec 96.875% (96.465%)\n",
      "Epoch: [39][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.0855 (0.1102)\tPrec 97.656% (96.245%)\n",
      "Epoch: [39][300/391]\tTime 0.039 (0.040)\tData 0.002 (0.002)\tLoss 0.1221 (0.1134)\tPrec 93.750% (96.179%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.2047 (0.2047)\tPrec 96.094% (96.094%)\n",
      " * Prec 89.840% \n",
      "best acc: 90.200000\n",
      "Epoch: [40][0/391]\tTime 0.281 (0.281)\tData 0.234 (0.234)\tLoss 0.0920 (0.0920)\tPrec 95.312% (95.312%)\n",
      "Epoch: [40][100/391]\tTime 0.041 (0.042)\tData 0.001 (0.004)\tLoss 0.0351 (0.1049)\tPrec 99.219% (96.504%)\n",
      "Epoch: [40][200/391]\tTime 0.038 (0.041)\tData 0.000 (0.003)\tLoss 0.1314 (0.1101)\tPrec 96.875% (96.308%)\n",
      "Epoch: [40][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.0917 (0.1114)\tPrec 96.875% (96.224%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.2146 (0.2146)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.860% \n",
      "best acc: 90.200000\n",
      "Epoch: [41][0/391]\tTime 0.232 (0.232)\tData 0.192 (0.192)\tLoss 0.1213 (0.1213)\tPrec 96.875% (96.875%)\n",
      "Epoch: [41][100/391]\tTime 0.038 (0.041)\tData 0.003 (0.003)\tLoss 0.1147 (0.1022)\tPrec 95.312% (96.403%)\n",
      "Epoch: [41][200/391]\tTime 0.038 (0.040)\tData 0.002 (0.003)\tLoss 0.1103 (0.1095)\tPrec 96.094% (96.175%)\n",
      "Epoch: [41][300/391]\tTime 0.041 (0.040)\tData 0.001 (0.002)\tLoss 0.1761 (0.1104)\tPrec 93.750% (96.164%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.1606 (0.1606)\tPrec 95.312% (95.312%)\n",
      " * Prec 89.870% \n",
      "best acc: 90.200000\n",
      "Epoch: [42][0/391]\tTime 0.270 (0.270)\tData 0.222 (0.222)\tLoss 0.1518 (0.1518)\tPrec 92.188% (92.188%)\n",
      "Epoch: [42][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0722 (0.1117)\tPrec 97.656% (96.117%)\n",
      "Epoch: [42][200/391]\tTime 0.043 (0.041)\tData 0.001 (0.003)\tLoss 0.0956 (0.1069)\tPrec 95.312% (96.393%)\n",
      "Epoch: [42][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.002)\tLoss 0.0924 (0.1100)\tPrec 96.094% (96.268%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.159 (0.159)\tLoss 0.2434 (0.2434)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.710% \n",
      "best acc: 90.200000\n",
      "Epoch: [43][0/391]\tTime 0.233 (0.233)\tData 0.193 (0.193)\tLoss 0.1127 (0.1127)\tPrec 94.531% (94.531%)\n",
      "Epoch: [43][100/391]\tTime 0.044 (0.043)\tData 0.002 (0.004)\tLoss 0.1143 (0.0975)\tPrec 96.094% (96.620%)\n",
      "Epoch: [43][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.003)\tLoss 0.1538 (0.1046)\tPrec 94.531% (96.339%)\n",
      "Epoch: [43][300/391]\tTime 0.040 (0.041)\tData 0.002 (0.002)\tLoss 0.0715 (0.1037)\tPrec 98.438% (96.397%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.1627 (0.1627)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.610% \n",
      "best acc: 90.200000\n",
      "Epoch: [44][0/391]\tTime 0.274 (0.274)\tData 0.231 (0.231)\tLoss 0.1103 (0.1103)\tPrec 96.875% (96.875%)\n",
      "Epoch: [44][100/391]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 0.1482 (0.1042)\tPrec 95.312% (96.310%)\n",
      "Epoch: [44][200/391]\tTime 0.040 (0.041)\tData 0.001 (0.003)\tLoss 0.1949 (0.1015)\tPrec 92.969% (96.490%)\n",
      "Epoch: [44][300/391]\tTime 0.041 (0.040)\tData 0.001 (0.002)\tLoss 0.0610 (0.1038)\tPrec 99.219% (96.473%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.1660 (0.1660)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.830% \n",
      "best acc: 90.200000\n",
      "Epoch: [45][0/391]\tTime 0.310 (0.310)\tData 0.263 (0.263)\tLoss 0.1593 (0.1593)\tPrec 94.531% (94.531%)\n",
      "Epoch: [45][100/391]\tTime 0.048 (0.044)\tData 0.002 (0.004)\tLoss 0.0309 (0.0928)\tPrec 99.219% (96.921%)\n",
      "Epoch: [45][200/391]\tTime 0.042 (0.042)\tData 0.002 (0.003)\tLoss 0.0200 (0.0935)\tPrec 100.000% (96.879%)\n",
      "Epoch: [45][300/391]\tTime 0.039 (0.041)\tData 0.002 (0.002)\tLoss 0.1800 (0.0963)\tPrec 92.969% (96.699%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.187 (0.187)\tLoss 0.1919 (0.1919)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.190% \n",
      "best acc: 90.200000\n",
      "Epoch: [46][0/391]\tTime 0.261 (0.261)\tData 0.216 (0.216)\tLoss 0.1060 (0.1060)\tPrec 93.750% (93.750%)\n",
      "Epoch: [46][100/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.0745 (0.0967)\tPrec 97.656% (96.620%)\n",
      "Epoch: [46][200/391]\tTime 0.037 (0.040)\tData 0.002 (0.003)\tLoss 0.0444 (0.0974)\tPrec 99.219% (96.599%)\n",
      "Epoch: [46][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.1020 (0.0983)\tPrec 96.875% (96.641%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.1759 (0.1759)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.880% \n",
      "best acc: 90.200000\n",
      "Epoch: [47][0/391]\tTime 0.263 (0.263)\tData 0.225 (0.225)\tLoss 0.0936 (0.0936)\tPrec 96.875% (96.875%)\n",
      "Epoch: [47][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0395 (0.0936)\tPrec 100.000% (96.682%)\n",
      "Epoch: [47][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.1231 (0.0953)\tPrec 96.875% (96.696%)\n",
      "Epoch: [47][300/391]\tTime 0.041 (0.040)\tData 0.001 (0.002)\tLoss 0.1193 (0.0965)\tPrec 96.094% (96.662%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.1327 (0.1327)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.230% \n",
      "best acc: 90.230000\n",
      "Epoch: [48][0/391]\tTime 0.260 (0.260)\tData 0.218 (0.218)\tLoss 0.0749 (0.0749)\tPrec 98.438% (98.438%)\n",
      "Epoch: [48][100/391]\tTime 0.038 (0.042)\tData 0.002 (0.004)\tLoss 0.0852 (0.0918)\tPrec 96.875% (96.883%)\n",
      "Epoch: [48][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.0549 (0.0912)\tPrec 97.656% (96.840%)\n",
      "Epoch: [48][300/391]\tTime 0.040 (0.041)\tData 0.001 (0.002)\tLoss 0.0491 (0.0932)\tPrec 99.219% (96.771%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.1978 (0.1978)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.670% \n",
      "best acc: 90.230000\n",
      "Epoch: [49][0/391]\tTime 0.268 (0.268)\tData 0.226 (0.226)\tLoss 0.0954 (0.0954)\tPrec 95.312% (95.312%)\n",
      "Epoch: [49][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0663 (0.0921)\tPrec 96.875% (96.666%)\n",
      "Epoch: [49][200/391]\tTime 0.045 (0.041)\tData 0.001 (0.003)\tLoss 0.1161 (0.0909)\tPrec 95.312% (96.867%)\n",
      "Epoch: [49][300/391]\tTime 0.037 (0.040)\tData 0.001 (0.002)\tLoss 0.0888 (0.0888)\tPrec 96.875% (96.919%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.1772 (0.1772)\tPrec 94.531% (94.531%)\n",
      " * Prec 89.850% \n",
      "best acc: 90.230000\n",
      "Epoch: [50][0/391]\tTime 0.237 (0.237)\tData 0.191 (0.191)\tLoss 0.0956 (0.0956)\tPrec 96.875% (96.875%)\n",
      "Epoch: [50][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.003)\tLoss 0.0749 (0.0796)\tPrec 96.875% (97.223%)\n",
      "Epoch: [50][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.002)\tLoss 0.0880 (0.0757)\tPrec 95.312% (97.474%)\n",
      "Epoch: [50][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.0349 (0.0738)\tPrec 99.219% (97.513%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.1605 (0.1605)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.610% \n",
      "best acc: 90.610000\n",
      "Epoch: [51][0/391]\tTime 0.301 (0.301)\tData 0.249 (0.249)\tLoss 0.0845 (0.0845)\tPrec 96.875% (96.875%)\n",
      "Epoch: [51][100/391]\tTime 0.039 (0.044)\tData 0.001 (0.004)\tLoss 0.0312 (0.0729)\tPrec 99.219% (97.641%)\n",
      "Epoch: [51][200/391]\tTime 0.039 (0.042)\tData 0.001 (0.003)\tLoss 0.0713 (0.0705)\tPrec 96.875% (97.683%)\n",
      "Epoch: [51][300/391]\tTime 0.040 (0.041)\tData 0.001 (0.002)\tLoss 0.0579 (0.0682)\tPrec 99.219% (97.724%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.1832 (0.1832)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.680% \n",
      "best acc: 90.680000\n",
      "Epoch: [52][0/391]\tTime 0.280 (0.280)\tData 0.236 (0.236)\tLoss 0.0366 (0.0366)\tPrec 99.219% (99.219%)\n",
      "Epoch: [52][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0818 (0.0669)\tPrec 98.438% (97.695%)\n",
      "Epoch: [52][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.0883 (0.0667)\tPrec 96.875% (97.699%)\n",
      "Epoch: [52][300/391]\tTime 0.040 (0.040)\tData 0.002 (0.002)\tLoss 0.0841 (0.0654)\tPrec 96.875% (97.778%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.1517 (0.1517)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.800% \n",
      "best acc: 90.800000\n",
      "Epoch: [53][0/391]\tTime 0.276 (0.276)\tData 0.229 (0.229)\tLoss 0.1887 (0.1887)\tPrec 94.531% (94.531%)\n",
      "Epoch: [53][100/391]\tTime 0.045 (0.042)\tData 0.001 (0.004)\tLoss 0.0868 (0.0653)\tPrec 97.656% (97.881%)\n",
      "Epoch: [53][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.0424 (0.0638)\tPrec 98.438% (97.854%)\n",
      "Epoch: [53][300/391]\tTime 0.038 (0.041)\tData 0.001 (0.002)\tLoss 0.0524 (0.0642)\tPrec 98.438% (97.859%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.1541 (0.1541)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.540% \n",
      "best acc: 90.800000\n",
      "Epoch: [54][0/391]\tTime 0.251 (0.251)\tData 0.207 (0.207)\tLoss 0.0657 (0.0657)\tPrec 98.438% (98.438%)\n",
      "Epoch: [54][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0373 (0.0592)\tPrec 99.219% (98.058%)\n",
      "Epoch: [54][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.0904 (0.0591)\tPrec 96.875% (98.041%)\n",
      "Epoch: [54][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.0921 (0.0611)\tPrec 97.656% (98.012%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.1559 (0.1559)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.960% \n",
      "best acc: 90.960000\n",
      "Epoch: [55][0/391]\tTime 0.279 (0.279)\tData 0.232 (0.232)\tLoss 0.0280 (0.0280)\tPrec 99.219% (99.219%)\n",
      "Epoch: [55][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0364 (0.0575)\tPrec 98.438% (98.058%)\n",
      "Epoch: [55][200/391]\tTime 0.039 (0.042)\tData 0.001 (0.003)\tLoss 0.1407 (0.0587)\tPrec 96.094% (98.060%)\n",
      "Epoch: [55][300/391]\tTime 0.048 (0.042)\tData 0.002 (0.002)\tLoss 0.0498 (0.0594)\tPrec 99.219% (98.007%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.1533 (0.1533)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.880% \n",
      "best acc: 90.960000\n",
      "Epoch: [56][0/391]\tTime 0.253 (0.253)\tData 0.213 (0.213)\tLoss 0.0306 (0.0306)\tPrec 99.219% (99.219%)\n",
      "Epoch: [56][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0168 (0.0611)\tPrec 99.219% (98.020%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.041)\tData 0.002 (0.003)\tLoss 0.0844 (0.0586)\tPrec 96.875% (98.076%)\n",
      "Epoch: [56][300/391]\tTime 0.040 (0.040)\tData 0.002 (0.002)\tLoss 0.0303 (0.0599)\tPrec 100.000% (98.017%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.1459 (0.1459)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.750% \n",
      "best acc: 90.960000\n",
      "Epoch: [57][0/391]\tTime 0.263 (0.263)\tData 0.230 (0.230)\tLoss 0.0512 (0.0512)\tPrec 99.219% (99.219%)\n",
      "Epoch: [57][100/391]\tTime 0.041 (0.043)\tData 0.001 (0.004)\tLoss 0.0547 (0.0575)\tPrec 97.656% (98.089%)\n",
      "Epoch: [57][200/391]\tTime 0.048 (0.042)\tData 0.003 (0.003)\tLoss 0.0960 (0.0560)\tPrec 96.875% (98.103%)\n",
      "Epoch: [57][300/391]\tTime 0.037 (0.041)\tData 0.002 (0.002)\tLoss 0.0337 (0.0573)\tPrec 99.219% (98.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.1675 (0.1675)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.810% \n",
      "best acc: 90.960000\n",
      "Epoch: [58][0/391]\tTime 0.260 (0.260)\tData 0.212 (0.212)\tLoss 0.0318 (0.0318)\tPrec 99.219% (99.219%)\n",
      "Epoch: [58][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.004)\tLoss 0.0250 (0.0566)\tPrec 100.000% (98.213%)\n",
      "Epoch: [58][200/391]\tTime 0.040 (0.042)\tData 0.001 (0.003)\tLoss 0.1424 (0.0605)\tPrec 97.656% (98.041%)\n",
      "Epoch: [58][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.002)\tLoss 0.0296 (0.0598)\tPrec 99.219% (98.061%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.1501 (0.1501)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.760% \n",
      "best acc: 90.960000\n",
      "Epoch: [59][0/391]\tTime 0.260 (0.260)\tData 0.216 (0.216)\tLoss 0.1029 (0.1029)\tPrec 95.312% (95.312%)\n",
      "Epoch: [59][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0706 (0.0629)\tPrec 97.656% (97.865%)\n",
      "Epoch: [59][200/391]\tTime 0.044 (0.041)\tData 0.001 (0.003)\tLoss 0.0287 (0.0589)\tPrec 99.219% (98.010%)\n",
      "Epoch: [59][300/391]\tTime 0.042 (0.040)\tData 0.001 (0.002)\tLoss 0.0820 (0.0586)\tPrec 96.875% (98.048%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.1372 (0.1372)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.840% \n",
      "best acc: 90.960000\n",
      "Epoch: [60][0/391]\tTime 0.273 (0.273)\tData 0.226 (0.226)\tLoss 0.0164 (0.0164)\tPrec 100.000% (100.000%)\n",
      "Epoch: [60][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0690 (0.0553)\tPrec 96.875% (98.051%)\n",
      "Epoch: [60][200/391]\tTime 0.040 (0.040)\tData 0.002 (0.003)\tLoss 0.0207 (0.0559)\tPrec 99.219% (98.049%)\n",
      "Epoch: [60][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.0970 (0.0558)\tPrec 97.656% (98.074%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.1633 (0.1633)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.910% \n",
      "best acc: 90.960000\n",
      "Epoch: [61][0/391]\tTime 0.246 (0.246)\tData 0.206 (0.206)\tLoss 0.0575 (0.0575)\tPrec 97.656% (97.656%)\n",
      "Epoch: [61][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.0754 (0.0503)\tPrec 96.875% (98.291%)\n",
      "Epoch: [61][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 0.0352 (0.0496)\tPrec 99.219% (98.317%)\n",
      "Epoch: [61][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.0611 (0.0496)\tPrec 98.438% (98.331%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.1679 (0.1679)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.870% \n",
      "best acc: 90.960000\n",
      "Epoch: [62][0/391]\tTime 0.264 (0.264)\tData 0.218 (0.218)\tLoss 0.0398 (0.0398)\tPrec 97.656% (97.656%)\n",
      "Epoch: [62][100/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.0958 (0.0632)\tPrec 97.656% (97.834%)\n",
      "Epoch: [62][200/391]\tTime 0.037 (0.041)\tData 0.001 (0.003)\tLoss 0.0231 (0.0595)\tPrec 99.219% (98.002%)\n",
      "Epoch: [62][300/391]\tTime 0.038 (0.041)\tData 0.001 (0.002)\tLoss 0.0733 (0.0582)\tPrec 97.656% (98.048%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.1503 (0.1503)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.830% \n",
      "best acc: 90.960000\n",
      "Epoch: [63][0/391]\tTime 0.249 (0.249)\tData 0.198 (0.198)\tLoss 0.1019 (0.1019)\tPrec 96.875% (96.875%)\n",
      "Epoch: [63][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0873 (0.0576)\tPrec 96.875% (98.159%)\n",
      "Epoch: [63][200/391]\tTime 0.045 (0.041)\tData 0.001 (0.003)\tLoss 0.0374 (0.0579)\tPrec 99.219% (98.177%)\n",
      "Epoch: [63][300/391]\tTime 0.038 (0.040)\tData 0.002 (0.002)\tLoss 0.0656 (0.0562)\tPrec 98.438% (98.243%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 0.1390 (0.1390)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.760% \n",
      "best acc: 90.960000\n",
      "Epoch: [64][0/391]\tTime 0.253 (0.253)\tData 0.220 (0.220)\tLoss 0.0799 (0.0799)\tPrec 97.656% (97.656%)\n",
      "Epoch: [64][100/391]\tTime 0.040 (0.042)\tData 0.001 (0.004)\tLoss 0.0714 (0.0515)\tPrec 98.438% (98.368%)\n",
      "Epoch: [64][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.1023 (0.0518)\tPrec 97.656% (98.348%)\n",
      "Epoch: [64][300/391]\tTime 0.040 (0.041)\tData 0.002 (0.002)\tLoss 0.0471 (0.0527)\tPrec 97.656% (98.316%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.1568 (0.1568)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.900% \n",
      "best acc: 90.960000\n",
      "Epoch: [65][0/391]\tTime 0.267 (0.267)\tData 0.221 (0.221)\tLoss 0.1238 (0.1238)\tPrec 96.875% (96.875%)\n",
      "Epoch: [65][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0263 (0.0522)\tPrec 100.000% (98.352%)\n",
      "Epoch: [65][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.0223 (0.0534)\tPrec 100.000% (98.286%)\n",
      "Epoch: [65][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.1380 (0.0550)\tPrec 96.875% (98.222%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.1552 (0.1552)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.790% \n",
      "best acc: 90.960000\n",
      "Epoch: [66][0/391]\tTime 0.259 (0.259)\tData 0.218 (0.218)\tLoss 0.0360 (0.0360)\tPrec 98.438% (98.438%)\n",
      "Epoch: [66][100/391]\tTime 0.038 (0.042)\tData 0.002 (0.004)\tLoss 0.0596 (0.0520)\tPrec 97.656% (98.329%)\n",
      "Epoch: [66][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.0205 (0.0547)\tPrec 100.000% (98.185%)\n",
      "Epoch: [66][300/391]\tTime 0.039 (0.040)\tData 0.001 (0.002)\tLoss 0.0189 (0.0529)\tPrec 100.000% (98.235%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.1602 (0.1602)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.750% \n",
      "best acc: 90.960000\n",
      "Epoch: [67][0/391]\tTime 0.298 (0.298)\tData 0.255 (0.255)\tLoss 0.0195 (0.0195)\tPrec 99.219% (99.219%)\n",
      "Epoch: [67][100/391]\tTime 0.047 (0.042)\tData 0.002 (0.004)\tLoss 0.0331 (0.0527)\tPrec 98.438% (98.205%)\n",
      "Epoch: [67][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 0.0677 (0.0548)\tPrec 97.656% (98.177%)\n",
      "Epoch: [67][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.002)\tLoss 0.0532 (0.0535)\tPrec 99.219% (98.225%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.1662 (0.1662)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.060% \n",
      "best acc: 91.060000\n",
      "Epoch: [68][0/391]\tTime 0.321 (0.321)\tData 0.272 (0.272)\tLoss 0.0218 (0.0218)\tPrec 100.000% (100.000%)\n",
      "Epoch: [68][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0423 (0.0600)\tPrec 98.438% (98.004%)\n",
      "Epoch: [68][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 0.0596 (0.0563)\tPrec 96.875% (98.158%)\n",
      "Epoch: [68][300/391]\tTime 0.038 (0.041)\tData 0.001 (0.002)\tLoss 0.0367 (0.0560)\tPrec 98.438% (98.108%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.1454 (0.1454)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.030% \n",
      "best acc: 91.060000\n",
      "Epoch: [69][0/391]\tTime 0.284 (0.284)\tData 0.240 (0.240)\tLoss 0.1127 (0.1127)\tPrec 96.875% (96.875%)\n",
      "Epoch: [69][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0481 (0.0565)\tPrec 98.438% (98.260%)\n",
      "Epoch: [69][200/391]\tTime 0.039 (0.041)\tData 0.001 (0.003)\tLoss 0.0209 (0.0544)\tPrec 100.000% (98.263%)\n",
      "Epoch: [69][300/391]\tTime 0.038 (0.041)\tData 0.002 (0.002)\tLoss 0.0396 (0.0526)\tPrec 98.438% (98.277%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.1470 (0.1470)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.850% \n",
      "best acc: 91.060000\n",
      "Epoch: [70][0/391]\tTime 0.279 (0.279)\tData 0.229 (0.229)\tLoss 0.0800 (0.0800)\tPrec 96.094% (96.094%)\n",
      "Epoch: [70][100/391]\tTime 0.038 (0.042)\tData 0.001 (0.004)\tLoss 0.0836 (0.0520)\tPrec 96.875% (98.213%)\n",
      "Epoch: [70][200/391]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.0245 (0.0524)\tPrec 98.438% (98.208%)\n",
      "Epoch: [70][300/391]\tTime 0.041 (0.040)\tData 0.003 (0.002)\tLoss 0.0411 (0.0526)\tPrec 98.438% (98.194%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.1435 (0.1435)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.000% \n",
      "best acc: 91.060000\n",
      "Epoch: [71][0/391]\tTime 0.294 (0.294)\tData 0.239 (0.239)\tLoss 0.1072 (0.1072)\tPrec 97.656% (97.656%)\n",
      "Epoch: [71][100/391]\tTime 0.041 (0.043)\tData 0.002 (0.004)\tLoss 0.0447 (0.0503)\tPrec 98.438% (98.221%)\n",
      "Epoch: [71][200/391]\tTime 0.043 (0.042)\tData 0.001 (0.003)\tLoss 0.0659 (0.0487)\tPrec 97.656% (98.305%)\n",
      "Epoch: [71][300/391]\tTime 0.034 (0.041)\tData 0.002 (0.002)\tLoss 0.0636 (0.0505)\tPrec 96.875% (98.287%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.1727 (0.1727)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.730% \n",
      "best acc: 91.060000\n",
      "Epoch: [72][0/391]\tTime 0.245 (0.245)\tData 0.206 (0.206)\tLoss 0.0190 (0.0190)\tPrec 100.000% (100.000%)\n",
      "Epoch: [72][100/391]\tTime 0.038 (0.041)\tData 0.001 (0.004)\tLoss 0.0742 (0.0516)\tPrec 96.875% (98.337%)\n",
      "Epoch: [72][200/391]\tTime 0.040 (0.041)\tData 0.001 (0.003)\tLoss 0.0475 (0.0513)\tPrec 98.438% (98.282%)\n",
      "Epoch: [72][300/391]\tTime 0.044 (0.040)\tData 0.001 (0.002)\tLoss 0.0234 (0.0510)\tPrec 99.219% (98.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.1911 (0.1911)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.910% \n",
      "best acc: 91.060000\n",
      "Epoch: [73][0/391]\tTime 0.269 (0.269)\tData 0.222 (0.222)\tLoss 0.0270 (0.0270)\tPrec 99.219% (99.219%)\n",
      "Epoch: [73][100/391]\tTime 0.036 (0.042)\tData 0.001 (0.004)\tLoss 0.0134 (0.0457)\tPrec 100.000% (98.445%)\n",
      "Epoch: [73][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.0255 (0.0465)\tPrec 100.000% (98.438%)\n",
      "Epoch: [73][300/391]\tTime 0.041 (0.040)\tData 0.002 (0.002)\tLoss 0.0213 (0.0455)\tPrec 100.000% (98.492%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.1827 (0.1827)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.780% \n",
      "best acc: 91.060000\n",
      "Epoch: [74][0/391]\tTime 0.232 (0.232)\tData 0.188 (0.188)\tLoss 0.0376 (0.0376)\tPrec 99.219% (99.219%)\n",
      "Epoch: [74][100/391]\tTime 0.043 (0.042)\tData 0.002 (0.004)\tLoss 0.0810 (0.0505)\tPrec 97.656% (98.275%)\n",
      "Epoch: [74][200/391]\tTime 0.040 (0.041)\tData 0.002 (0.003)\tLoss 0.0754 (0.0516)\tPrec 96.875% (98.263%)\n",
      "Epoch: [74][300/391]\tTime 0.039 (0.041)\tData 0.001 (0.002)\tLoss 0.0326 (0.0503)\tPrec 98.438% (98.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.2071 (0.2071)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.880% \n",
      "best acc: 91.060000\n",
      "Epoch: [75][0/391]\tTime 0.260 (0.260)\tData 0.216 (0.216)\tLoss 0.0247 (0.0247)\tPrec 99.219% (99.219%)\n",
      "Epoch: [75][100/391]\tTime 0.041 (0.042)\tData 0.001 (0.004)\tLoss 0.0492 (0.0505)\tPrec 97.656% (98.468%)\n",
      "Epoch: [75][200/391]\tTime 0.043 (0.041)\tData 0.001 (0.003)\tLoss 0.0433 (0.0489)\tPrec 98.438% (98.457%)\n",
      "Epoch: [75][300/391]\tTime 0.040 (0.041)\tData 0.001 (0.002)\tLoss 0.0511 (0.0503)\tPrec 98.438% (98.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.1570 (0.1570)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.740% \n",
      "best acc: 91.060000\n",
      "Epoch: [76][0/391]\tTime 0.274 (0.274)\tData 0.242 (0.242)\tLoss 0.0311 (0.0311)\tPrec 99.219% (99.219%)\n",
      "Epoch: [76][100/391]\tTime 0.038 (0.043)\tData 0.002 (0.004)\tLoss 0.0510 (0.0482)\tPrec 96.875% (98.438%)\n",
      "Epoch: [76][200/391]\tTime 0.042 (0.041)\tData 0.001 (0.003)\tLoss 0.0813 (0.0493)\tPrec 96.875% (98.406%)\n",
      "Epoch: [76][300/391]\tTime 0.038 (0.041)\tData 0.001 (0.002)\tLoss 0.0918 (0.0478)\tPrec 96.875% (98.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.1858 (0.1858)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.860% \n",
      "best acc: 91.060000\n",
      "Epoch: [77][0/391]\tTime 0.244 (0.244)\tData 0.198 (0.198)\tLoss 0.0520 (0.0520)\tPrec 98.438% (98.438%)\n",
      "Epoch: [77][100/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.0778 (0.0490)\tPrec 97.656% (98.383%)\n",
      "Epoch: [77][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.003)\tLoss 0.0205 (0.0490)\tPrec 99.219% (98.336%)\n",
      "Epoch: [77][300/391]\tTime 0.044 (0.041)\tData 0.002 (0.002)\tLoss 0.0724 (0.0483)\tPrec 96.094% (98.370%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.1386 (0.1386)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.900% \n",
      "best acc: 91.060000\n",
      "Epoch: [78][0/391]\tTime 0.248 (0.248)\tData 0.206 (0.206)\tLoss 0.0494 (0.0494)\tPrec 98.438% (98.438%)\n",
      "Epoch: [78][100/391]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.0552 (0.0478)\tPrec 98.438% (98.368%)\n",
      "Epoch: [78][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 0.0472 (0.0499)\tPrec 97.656% (98.371%)\n",
      "Epoch: [78][300/391]\tTime 0.046 (0.040)\tData 0.001 (0.002)\tLoss 0.1194 (0.0493)\tPrec 94.531% (98.412%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.1770 (0.1770)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.700% \n",
      "best acc: 91.060000\n",
      "Epoch: [79][0/391]\tTime 0.294 (0.294)\tData 0.251 (0.251)\tLoss 0.0208 (0.0208)\tPrec 100.000% (100.000%)\n",
      "Epoch: [79][100/391]\tTime 0.040 (0.043)\tData 0.001 (0.004)\tLoss 0.0683 (0.0467)\tPrec 96.094% (98.383%)\n",
      "Epoch: [79][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.003)\tLoss 0.0377 (0.0474)\tPrec 98.438% (98.371%)\n",
      "Epoch: [79][300/391]\tTime 0.041 (0.041)\tData 0.001 (0.003)\tLoss 0.0354 (0.0468)\tPrec 100.000% (98.432%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.1724 (0.1724)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.850% \n",
      "best acc: 91.060000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [20, 50]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1  \n",
    "\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-3\n",
    "epochs = 80\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name) + '_modified'\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d18b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9106/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant_modified/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e88f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n",
      "prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "# print(\"1st convolution's input size:\", save_output.outputs[0][0].size())\n",
    "# print(\"2nd convolution's input size:\", save_output.outputs[1][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8064e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 QuantConv2d(\n",
      "  3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "1 3 QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "2 7 QuantConv2d(\n",
      "  64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "3 10 QuantConv2d(\n",
      "  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "4 14 QuantConv2d(\n",
      "  128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "5 17 QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "6 20 QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "7 24 QuantConv2d(\n",
      "  256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "8 27 QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "9 29 QuantConv2d(\n",
      "  8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "10 33 QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "11 36 QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n",
      "12 39 QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for iter, val in enumerate(model.features):\n",
    "    if isinstance(val,QuantConv2d):\n",
    "        print (i, iter, val)\n",
    "        print (\"\\n\")\n",
    "        i=i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cfc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)\n",
    "weight_int = weight_q/w_delta\n",
    "#print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bit = 4\n",
    "x = save_output.outputs[8][0] # input of the 2nd conv layer\n",
    "x_alpha = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "#print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a514a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Padding before Convolution #######\n",
    "x_pad = torch.zeros(128, 8, 6, 6).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "x_pad[ : ,  :, 1:5, 1:5] = x_int.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c99cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "\n",
    "output_int =  conv_int(x_pad)\n",
    "output_recovered = output_int*w_delta*x_delta\n",
    "#print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10dfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, bias = False)\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, padding=(1, 1), kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.features[27].weight_q\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "#print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557279f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1956, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref[0] - output_recovered[0] )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1ff5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_pad[0] \n",
    "X = torch.reshape(X, (X.size(0), -1))\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(int(X[7-j,i].item()+0.001))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "#         file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6865ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_precision = 4\n",
    "file = open('activation_dec.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_dec = str(int(X[7-j,i].item()+0.001))\n",
    "        file.write(X_dec)        \n",
    "        file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08fd609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 36])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x_pad[0] \n",
    "# X = torch.reshape(X, (4,72,-1))\n",
    "X = torch.reshape(X, (X.size(0), -1))\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "836a68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_int.size() # 8, 8 , 3, 3\n",
    "W = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))\n",
    "W.size() # 8, 8, 9\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('weight.txt', 'w') #write to file\n",
    "file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "\n",
    "W_temp=0\n",
    "for kij in range(9):\n",
    "    for i in range(W.size(0)):  # Column #\n",
    "        for j in range(W.size(1)): # row #\n",
    "            #W_bin = '{0:04b}'.format(int(W[i,7-j].item()+0.001))\n",
    "            if (W[i,7-j,kij].item()<0):\n",
    "                W_temp=W[i,7-j,kij].item()+(2**bit_precision)\n",
    "            else:\n",
    "                W_temp=W[i,7-j,kij].item()\n",
    "            W_bin = '{0:04b}'.format(int(W_temp+0.001))\n",
    "            for k in range(bit_precision):\n",
    "                file.write(str(W_bin[k]))        \n",
    "#             file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f09baef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0000, -7.0000, -7.0000, -7.0000, -6.0000, -7.0000, -7.0000, -5.0000],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_int.size()\n",
    "W = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))\n",
    "W.size()\n",
    "W[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8f9ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7 0 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 6 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 0 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "0 3 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 1 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 0 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 7 0 1.0 1 1.0\n",
      "\n",
      "\n",
      "1 6 0 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 5 0 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 4 0 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "1 3 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 2 0 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "1 1 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 0 0 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 7 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 6 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 5 0 2.0 2 2.0\n",
      "\n",
      "\n",
      "2 4 0 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "2 3 0 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "2 2 0 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 1 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 0 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 7 0 2.0 2 2.0\n",
      "\n",
      "\n",
      "3 6 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 5 0 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "3 4 0 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "3 3 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 2 0 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "3 1 0 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 0 0 6.0 6 6.0\n",
      "\n",
      "\n",
      "4 7 0 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 6 0 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 5 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "4 4 0 2.0 2 2.0\n",
      "\n",
      "\n",
      "4 3 0 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "4 2 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "4 1 0 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "4 0 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 7 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 6 0 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 5 0 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "5 4 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 3 0 1.0 1 1.0\n",
      "\n",
      "\n",
      "5 2 0 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "5 1 0 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 0 0 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "6 7 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 6 0 5.0 5 5.0\n",
      "\n",
      "\n",
      "6 5 0 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 4 0 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "6 3 0 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 2 0 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 1 0 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 0 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 7 0 1.0 1 1.0\n",
      "\n",
      "\n",
      "7 6 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 5 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 4 0 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "7 3 0 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 2 0 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 1 0 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "7 0 0 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "0 7 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 6 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 3 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 1 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 0 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 7 1 1.0 1 1.0\n",
      "\n",
      "\n",
      "1 6 1 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 5 1 4.0 4 4.0\n",
      "\n",
      "\n",
      "1 4 1 2.0 2 2.0\n",
      "\n",
      "\n",
      "1 3 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 2 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 1 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 0 1 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 7 1 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 6 1 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "2 5 1 5.0 5 5.0\n",
      "\n",
      "\n",
      "2 4 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 3 1 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "2 2 1 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 1 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 0 1 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "3 7 1 3.0 3 3.0\n",
      "\n",
      "\n",
      "3 6 1 1.0 1 1.0\n",
      "\n",
      "\n",
      "3 5 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "3 4 1 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "3 3 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "3 2 1 6.0 6 6.0\n",
      "\n",
      "\n",
      "3 1 1 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "3 0 1 4.0 4 4.0\n",
      "\n",
      "\n",
      "4 7 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 6 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 5 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 4 1 2.0 2 2.0\n",
      "\n",
      "\n",
      "4 3 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "4 2 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 1 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 0 1 5.0 5 5.0\n",
      "\n",
      "\n",
      "5 7 1 3.0 3 3.0\n",
      "\n",
      "\n",
      "5 6 1 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "5 5 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 4 1 1.0 1 1.0\n",
      "\n",
      "\n",
      "5 3 1 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "5 2 1 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "5 1 1 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 0 1 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 7 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 6 1 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "6 5 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 4 1 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "6 3 1 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "6 2 1 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 1 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 0 1 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 7 1 6.0 6 6.0\n",
      "\n",
      "\n",
      "7 6 1 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 5 1 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 4 1 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 3 1 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 2 1 1.0 1 1.0\n",
      "\n",
      "\n",
      "7 1 1 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 0 1 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 7 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 6 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 3 2 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 2 2 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 1 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 7 2 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 6 2 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 5 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "1 4 2 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "1 3 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 2 2 5.0 5 5.0\n",
      "\n",
      "\n",
      "1 1 2 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 0 2 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 7 2 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 6 2 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 5 2 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "2 4 2 5.0 5 5.0\n",
      "\n",
      "\n",
      "2 3 2 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "2 2 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 1 2 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 0 2 5.0 5 5.0\n",
      "\n",
      "\n",
      "3 7 2 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 6 2 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "3 5 2 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "3 4 2 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "3 3 2 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "3 2 2 5.0 5 5.0\n",
      "\n",
      "\n",
      "3 1 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 0 2 1.0 1 1.0\n",
      "\n",
      "\n",
      "4 7 2 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 6 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 5 2 6.0 6 6.0\n",
      "\n",
      "\n",
      "4 4 2 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 3 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "4 2 2 1.0 1 1.0\n",
      "\n",
      "\n",
      "4 1 2 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 0 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 7 2 0.0 0 0.0\n",
      "\n",
      "\n",
      "5 6 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 5 2 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 4 2 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "5 3 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 2 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 1 2 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "5 0 2 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 7 2 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 6 2 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "6 5 2 1.0 1 1.0\n",
      "\n",
      "\n",
      "6 4 2 1.0 1 1.0\n",
      "\n",
      "\n",
      "6 3 2 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 2 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "6 1 2 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 0 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 7 2 2.0 2 2.0\n",
      "\n",
      "\n",
      "7 6 2 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 5 2 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "7 4 2 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 3 2 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "7 2 2 2.0 2 2.0\n",
      "\n",
      "\n",
      "7 1 2 1.0 1 1.0\n",
      "\n",
      "\n",
      "7 0 2 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 7 3 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 6 3 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 3 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 3 3 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 2 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 1 3 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 0 3 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "1 7 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "1 6 3 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 5 3 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "1 4 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 3 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 2 3 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 1 3 2.0 2 2.0\n",
      "\n",
      "\n",
      "1 0 3 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 7 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 6 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 5 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "2 4 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 3 3 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "2 2 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 1 3 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "2 0 3 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 7 3 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "3 6 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 5 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "3 4 3 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "3 3 3 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "3 2 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "3 1 3 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 0 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 7 3 0.0 0 0.0\n",
      "\n",
      "\n",
      "4 6 3 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "4 5 3 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 4 3 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "4 3 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "4 2 3 0.0 0 0.0\n",
      "\n",
      "\n",
      "4 1 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 0 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 7 3 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 6 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 5 3 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 4 3 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "5 3 3 3.0 3 3.0\n",
      "\n",
      "\n",
      "5 2 3 0.0 0 0.0\n",
      "\n",
      "\n",
      "5 1 3 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 0 3 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "6 7 3 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "6 6 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 5 3 0.0 0 0.0\n",
      "\n",
      "\n",
      "6 4 3 3.0 3 3.0\n",
      "\n",
      "\n",
      "6 3 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 2 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "6 1 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "6 0 3 1.0 1 1.0\n",
      "\n",
      "\n",
      "7 7 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "7 6 3 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 5 3 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 4 3 3.0 3 3.0\n",
      "\n",
      "\n",
      "7 3 3 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "7 2 3 5.0 5 5.0\n",
      "\n",
      "\n",
      "7 1 3 2.0 2 2.0\n",
      "\n",
      "\n",
      "7 0 3 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 7 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 6 4 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "0 5 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 3 4 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 2 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 1 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 7 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 6 4 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 5 4 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "1 4 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 3 4 2.0 2 2.0\n",
      "\n",
      "\n",
      "1 2 4 1.0 1 1.0\n",
      "\n",
      "\n",
      "1 1 4 1.0 1 1.0\n",
      "\n",
      "\n",
      "1 0 4 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "2 7 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 6 4 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 5 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 4 4 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 3 4 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 2 4 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 1 4 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 0 4 6.0 6 6.0\n",
      "\n",
      "\n",
      "3 7 4 2.0 2 2.0\n",
      "\n",
      "\n",
      "3 6 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 5 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 4 4 6.0 6 6.0\n",
      "\n",
      "\n",
      "3 3 4 1.0 1 1.0\n",
      "\n",
      "\n",
      "3 2 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 1 4 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "3 0 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 7 4 4.0 4 4.0\n",
      "\n",
      "\n",
      "4 6 4 0.0 0 0.0\n",
      "\n",
      "\n",
      "4 5 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "4 4 4 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "4 3 4 6.0 6 6.0\n",
      "\n",
      "\n",
      "4 2 4 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 1 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 0 4 3.0 3 3.0\n",
      "\n",
      "\n",
      "5 7 4 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 6 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "5 5 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 4 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 3 4 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "5 2 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "5 1 4 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 0 4 2.0 2 2.0\n",
      "\n",
      "\n",
      "6 7 4 2.0 2 2.0\n",
      "\n",
      "\n",
      "6 6 4 4.0 4 4.0\n",
      "\n",
      "\n",
      "6 5 4 0.0 0 0.0\n",
      "\n",
      "\n",
      "6 4 4 1.0 1 1.0\n",
      "\n",
      "\n",
      "6 3 4 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 2 4 0.0 0 0.0\n",
      "\n",
      "\n",
      "6 1 4 4.0 4 4.0\n",
      "\n",
      "\n",
      "6 0 4 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 7 4 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 6 4 3.0 3 3.0\n",
      "\n",
      "\n",
      "7 5 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "7 4 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "7 3 4 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 2 4 6.0 6 6.0\n",
      "\n",
      "\n",
      "7 1 4 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "7 0 4 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 7 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "0 6 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "0 5 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 3 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "0 1 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "1 7 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 6 5 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "1 5 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 4 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 3 5 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "1 2 5 5.0 5 5.0\n",
      "\n",
      "\n",
      "1 1 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 0 5 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 7 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 6 5 5.0 5 5.0\n",
      "\n",
      "\n",
      "2 5 5 0.0 0 0.0\n",
      "\n",
      "\n",
      "2 4 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 3 5 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 2 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "2 1 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "2 0 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 7 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 6 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "3 5 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 4 5 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 3 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 2 5 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 1 5 1.0 1 1.0\n",
      "\n",
      "\n",
      "3 0 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 7 5 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 6 5 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "4 5 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 4 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "4 3 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 2 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 1 5 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 0 5 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "5 7 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 6 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 5 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 4 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 3 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "5 2 5 0.0 0 0.0\n",
      "\n",
      "\n",
      "5 1 5 5.0 5 5.0\n",
      "\n",
      "\n",
      "5 0 5 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 7 5 1.0 1 1.0\n",
      "\n",
      "\n",
      "6 6 5 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 5 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 4 5 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "6 3 5 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 2 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "6 1 5 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 0 5 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 7 5 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 6 5 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "7 5 5 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "7 4 5 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "7 3 5 4.0 4 4.0\n",
      "\n",
      "\n",
      "7 2 5 6.0 6 6.0\n",
      "\n",
      "\n",
      "7 1 5 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 0 5 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "0 7 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 6 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 4 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 3 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 1 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 0 6 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 7 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 6 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 5 6 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 4 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "1 3 6 6.0 6 6.0\n",
      "\n",
      "\n",
      "1 2 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 1 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "1 0 6 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 7 6 2.0 2 2.0\n",
      "\n",
      "\n",
      "2 6 6 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 5 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 4 6 4.0 4 4.0\n",
      "\n",
      "\n",
      "2 3 6 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "2 2 6 0.0 0 0.0\n",
      "\n",
      "\n",
      "2 1 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "2 0 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "3 7 6 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "3 6 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 5 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 4 6 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "3 3 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "3 2 6 0.0 0 0.0\n",
      "\n",
      "\n",
      "3 1 6 0.0 0 0.0\n",
      "\n",
      "\n",
      "3 0 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 7 6 1.0 1 1.0\n",
      "\n",
      "\n",
      "4 6 6 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "4 5 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 4 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 3 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 2 6 0.0 0 0.0\n",
      "\n",
      "\n",
      "4 1 6 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "4 0 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 7 6 3.0 3 3.0\n",
      "\n",
      "\n",
      "5 6 6 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "5 5 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 4 6 6.0 6 6.0\n",
      "\n",
      "\n",
      "5 3 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 2 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 1 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "5 0 6 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "6 7 6 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "6 6 6 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 5 6 1.0 1 1.0\n",
      "\n",
      "\n",
      "6 4 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 3 6 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "6 2 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 1 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "6 0 6 5.0 5 5.0\n",
      "\n",
      "\n",
      "7 7 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 6 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 5 6 2.0 2 2.0\n",
      "\n",
      "\n",
      "7 4 6 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "7 3 6 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "7 2 6 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 1 6 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "7 0 6 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 7 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 6 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 7 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 3 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 7 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "0 1 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 7 7 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "1 6 7 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "1 5 7 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 4 7 5.0 5 5.0\n",
      "\n",
      "\n",
      "1 3 7 6.0 6 6.0\n",
      "\n",
      "\n",
      "1 2 7 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 1 7 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "1 0 7 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 7 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 6 7 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 5 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 4 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 3 7 5.0 5 5.0\n",
      "\n",
      "\n",
      "2 2 7 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "2 1 7 1.0 1 1.0\n",
      "\n",
      "\n",
      "2 0 7 0.0 0 0.0\n",
      "\n",
      "\n",
      "3 7 7 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "3 6 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 5 7 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "3 4 7 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 3 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "3 2 7 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "3 1 7 2.0 2 2.0\n",
      "\n",
      "\n",
      "3 0 7 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 7 7 2.0 2 2.0\n",
      "\n",
      "\n",
      "4 6 7 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "4 5 7 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 4 7 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "4 3 7 5.0 5 5.0\n",
      "\n",
      "\n",
      "4 2 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 1 7 3.0 3 3.0\n",
      "\n",
      "\n",
      "4 0 7 5.0 5 5.0\n",
      "\n",
      "\n",
      "5 7 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 6 7 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "5 5 7 2.0 2 2.0\n",
      "\n",
      "\n",
      "5 4 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 3 7 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "5 2 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 1 7 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "5 0 7 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "6 7 7 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "6 6 7 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 5 7 4.0 4 4.0\n",
      "\n",
      "\n",
      "6 4 7 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 3 7 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "6 2 7 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 1 7 4.0 4 4.0\n",
      "\n",
      "\n",
      "6 0 7 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 7 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 6 7 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 5 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 4 7 0.0 0 0.0\n",
      "\n",
      "\n",
      "7 3 7 5.0 5 5.0\n",
      "\n",
      "\n",
      "7 2 7 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 1 7 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "7 0 7 2.0 2 2.0\n",
      "\n",
      "\n",
      "0 7 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "0 6 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 5 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 4 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 3 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 2 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 1 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "0 0 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 7 8 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 6 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "1 5 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 4 8 6.0 6 6.0\n",
      "\n",
      "\n",
      "1 3 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "1 2 8 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "1 1 8 3.0 3 3.0\n",
      "\n",
      "\n",
      "1 0 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 7 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "2 6 8 3.0 3 3.0\n",
      "\n",
      "\n",
      "2 5 8 0.0 0 0.0\n",
      "\n",
      "\n",
      "2 4 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "2 3 8 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "2 2 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "2 1 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "2 0 8 0.0 0 0.0\n",
      "\n",
      "\n",
      "3 7 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "3 6 8 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 5 8 2.0 2 2.0\n",
      "\n",
      "\n",
      "3 4 8 4.0 4 4.0\n",
      "\n",
      "\n",
      "3 3 8 1.0 1 1.0\n",
      "\n",
      "\n",
      "3 2 8 10.0 -6 -6.0\n",
      "\n",
      "\n",
      "3 1 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "3 0 8 6.0 6 6.0\n",
      "\n",
      "\n",
      "4 7 8 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "4 6 8 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "4 5 8 1.0 1 1.0\n",
      "\n",
      "\n",
      "4 4 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 3 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "4 2 8 2.0 2 2.0\n",
      "\n",
      "\n",
      "4 1 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "4 0 8 2.0 2 2.0\n",
      "\n",
      "\n",
      "5 7 8 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "5 6 8 2.0 2 2.0\n",
      "\n",
      "\n",
      "5 5 8 3.0 3 3.0\n",
      "\n",
      "\n",
      "5 4 8 4.0 4 4.0\n",
      "\n",
      "\n",
      "5 3 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "5 2 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 1 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "5 0 8 13.0 -3 -3.0\n",
      "\n",
      "\n",
      "6 7 8 11.0 -5 -5.0\n",
      "\n",
      "\n",
      "6 6 8 0.0 0 0.0\n",
      "\n",
      "\n",
      "6 5 8 4.0 4 4.0\n",
      "\n",
      "\n",
      "6 4 8 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 3 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "6 2 8 15.0 -1 -1.0\n",
      "\n",
      "\n",
      "6 1 8 6.0 6 6.0\n",
      "\n",
      "\n",
      "6 0 8 14.0 -2 -2.0\n",
      "\n",
      "\n",
      "7 7 8 -0.0 0 -0.0\n",
      "\n",
      "\n",
      "7 6 8 9.000000476837158 -7 -6.999999523162842\n",
      "\n",
      "\n",
      "7 5 8 3.0 3 3.0\n",
      "\n",
      "\n",
      "7 4 8 12.0 -4 -4.0\n",
      "\n",
      "\n",
      "7 3 8 5.0 5 5.0\n",
      "\n",
      "\n",
      "7 2 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 1 8 6.999999523162842 7 6.999999523162842\n",
      "\n",
      "\n",
      "7 0 8 6.0 6 6.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bit_precision = 4\n",
    "file = open('weight_dec.txt', 'w') #write to file\n",
    "file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "\n",
    "W_temp=0\n",
    "for kij in range(9):\n",
    "    for i in range(W.size(0)):  # Column #\n",
    "        for j in range(W.size(1)): # row #\n",
    "            #W_bin = '{0:04b}'.format(int(W[i,7-j].item()+0.001))\n",
    "            if (W[i,7-j,kij].item()<0):\n",
    "                W_temp=W[i,7-j,kij].item()+16\n",
    "            else:\n",
    "                W_temp=W[i,7-j,kij].item()\n",
    "            W_dec = int(W_temp+0.001)\n",
    "            if (W_dec>=8) :  \n",
    "                W_dec = (W_dec - 16) \n",
    "            file.write(str(W_dec)) \n",
    "            print(i,7-j,kij, W_temp, W_dec, W[i,7-j,kij].item())\n",
    "            print(\"\\n\")\n",
    "            file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5601447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_int.size()\n",
    "O = output_int[0]\n",
    "O = torch.reshape(O, (O.size(0), -1))\n",
    "O.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3906c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('psum.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(O.size(1)):  # time step\n",
    "    for j in range(O.size(0)): # Column #\n",
    "        if (O[7-j,i].item()<0):\n",
    "            O_temp=O[7-j,i].item()+(2**bit_precision)\n",
    "        else:\n",
    "            O_temp=O[7-j,i].item()\n",
    "        O_bin = '{0:016b}'.format(int(O_temp+0.001))\n",
    "#         O_bin = str(int(O_temp+0.001))\n",
    "        #psum_tile_bin = '{0:016b}'.format(int(psum_tile[7-j,i].item()+0.001))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(O_bin[k])\n",
    "#         file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca60853",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('psum_dec.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(O.size(1)):  # time step\n",
    "    for j in range(O.size(0)): # Column #\n",
    "        if (O[7-j,i].item()<0):\n",
    "            O_temp=O[7-j,i].item()+(2**bit_precision)\n",
    "        else:\n",
    "            O_temp=O[7-j,i].item()\n",
    "        O_dec = int(O_temp+0.001)\n",
    "        if(O_dec>=2**(bit_precision-1)):\n",
    "            O_dec=O_dec-(2**bit_precision)\n",
    "        file.write(str(O_dec))\n",
    "        file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ba4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('output.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(O.size(1)):  # time step\n",
    "    for j in range(O.size(0)): # Column #\n",
    "        if (O[7-j,i].item()<0):\n",
    "            #O_temp=O[7-j,i].item()+(2**bit_precision)\n",
    "            O_temp=0\n",
    "        else:\n",
    "            O_temp=O[7-j,i].item()\n",
    "        O_bin = '{0:016b}'.format(int(O_temp+0.001))\n",
    "#         O_bin = str(int(O_temp+0.001))\n",
    "        #psum_tile_bin = '{0:016b}'.format(int(psum_tile[7-j,i].item()+0.001))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(O_bin[k])\n",
    "#         file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f747809",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('output_dec.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(O.size(1)):  # time step\n",
    "    for j in range(O.size(0)): # Column #\n",
    "        if (O[7-j,i].item()<0):\n",
    "            O_temp=0\n",
    "        else:\n",
    "            O_temp=O[7-j,i].item()\n",
    "        O_dec = int(O_temp+0.001)\n",
    "        #if(O_dec>=2**(bit_precision-1)):\n",
    "        #    O_dec=O_dec-(2**bit_precision)\n",
    "        file.write(str(O_dec))\n",
    "        file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc6a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
